{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1x0PRJii9C33"
   },
   "source": [
    "## Summary of this Notebook\n",
    "\n",
    "This notebook is a guide to the paper ([archiv](https://arxiv.org/pdf/1801.07593.pdf))\n",
    "\n",
    "> ```Brian Zhang, Blake Lemoine and Margaret Mitchell. Mitigating Unwanted Biases with Adversarial Learning. AAAI Conference on AI, Ethics and Society, 2018.```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BnQsDuO9FSt"
   },
   "source": [
    "## Intro \n",
    "\n",
    "Embeddings are a powerful mechanism for projecting a discrete variable (e.g. words, locales, urls) into a multi-dimensional real valued space.  Several strong methods have been developed for learning embeddings.  One example is the [Skipgram](http://www.cs.brandeis.edu/~marc/misc/proceedings/lrec-2006/pdf/357_pdf.pdf) algorithm.  In that algorithm the surrounding context is used to predict the presence of a word.  Unfortunately, much real world textual data has subtle bias that machine learning algorithms will implicitly include in the embeddings created from that data.  This bias can be illustrated by performing a word analogy task using the learned embeddings.\n",
    "\n",
    "It is worth noting that the usages of terms like _fair_ and _bias_ are used in this notebook in the context of a particular definition of fairness sometimes referred to as \"Demographic Parity\" or \"Equality of Outcomes\" ([Hardt et. al 2016](http://papers.nips.cc/paper/6373-equality-of-opportunity-in-supervised-learning)).  This definition of fairness effectively says that any relationship at all between a variable of interest and a _protected variable_ is an example of unwanted bias.  Other definitions of fairness such as \"Equality of Odds\" can be employed when there is believed to be some form of proper relationship between the variable of interest and the protected variable.  However, all uses of _fair_ and _bias_ here should be interpreted in the context of \"Demographic Parity\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G50xVJvB92z9"
   },
   "source": [
    "First, we'll import all the packages that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOrI3fGc87cz"
   },
   "outputs": [],
   "source": [
    "!pip install -U gensim~=3.2.0\n",
    "import gensim\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "!pip install --upgrade-strategy=only-if-needed tensorflow~=1.6.0rc0\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrGSp0fA9W8B"
   },
   "source": [
    "Now we'll sync the data for the colab to a tmp directory from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_B4Madl_5xp"
   },
   "outputs": [],
   "source": [
    "project_id = 'mledu-fairness'\n",
    "!gcloud config set project {project_id}\n",
    "\n",
    "gcs_bucket_name = 'mledu-fairness/colabs/debias_word_embeddings'\n",
    "local_dir_name = '/tmp/debias_word_embeddings'\n",
    "if not os.path.exists(local_dir_name):\n",
    "  print \"creating dir %s\" % local_dir_name\n",
    "  !mkdir {local_dir_name}\n",
    "  \n",
    "!gsutil rsync gs://{gcs_bucket_name} {local_dir_name}\n",
    "\n",
    "!ls -al {local_dir_name}  \n",
    "\n",
    "WORD2VEC_FILE = os.path.join(local_dir_name, \"GoogleNews-vectors-negative300.bin.gz\")\n",
    "ANALOGIES_FILE = os.path.join(local_dir_name, \"questions-words.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztVqGJ8xGF1b"
   },
   "outputs": [],
   "source": [
    "def load_word2vec_format(f, max_num_words=None):\n",
    "  \"\"\"Loads word2vec data from a file handle.\n",
    "\n",
    "  Similar to gensim.models.keyedvectors.KeyedVectors.load_word2vec_format\n",
    "  but takes a file handle as input rather than a filename. This lets us use\n",
    "  GFile. Also only accepts binary files.\n",
    "\n",
    "  Args:\n",
    "    f: file handle\n",
    "    max_num_words: number of words to load. If None, load all.\n",
    "\n",
    "  Returns:\n",
    "    Word2vec data as keyedvectors.EuclideanKeyedVectors.\n",
    "  \"\"\"\n",
    "  header = f.readline()\n",
    "  vocab_size, vector_size = (\n",
    "      int(x) for x in header.rstrip().split())  # throws for invalid file format\n",
    "  print \"vector_size =  %d\" % vector_size\n",
    "  result = gensim.models.keyedvectors.EuclideanKeyedVectors()\n",
    "  num_words = 0\n",
    "  result.vector_size = vector_size\n",
    "  result.syn0 = np.zeros((vocab_size, vector_size), dtype=np.float32)\n",
    "  \n",
    "  def add_word(word, weights):\n",
    "    word_id = len(result.vocab)\n",
    "    if word in result.vocab:\n",
    "      print(\"duplicate word '%s', ignoring all but first\", word)\n",
    "      return\n",
    "    result.vocab[word] = gensim.models.keyedvectors.Vocab(\n",
    "        index=word_id, count=vocab_size - word_id)\n",
    "    result.syn0[word_id] = weights\n",
    "    result.index2word.append(word)\n",
    "\n",
    "  if max_num_words and max_num_words < vocab_size:\n",
    "    num_embeddings = max_num_words\n",
    "  else:\n",
    "    num_embeddings = vocab_size\n",
    "  print \"Loading %d embeddings\" % num_embeddings\n",
    "  \n",
    "  binary_len = np.dtype(np.float32).itemsize * vector_size\n",
    "  for _ in xrange(vocab_size):\n",
    "    # mixed text and binary: read text first, then binary\n",
    "    word = []\n",
    "    while True:\n",
    "      ch = f.read(1)\n",
    "      if ch == b' ':\n",
    "        break\n",
    "      if ch == b'':\n",
    "        raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "      if ch != b'\\n':  # ignore newlines in front of words (some binary files have)\n",
    "        word.append(ch)\n",
    "    word = gensim.utils.to_unicode(b''.join(word), encoding='utf-8', errors='strict')\n",
    "    weights = np.frombuffer(f.read(binary_len), dtype=np.float32)\n",
    "    add_word(word, weights)\n",
    "    num_words = num_words + 1\n",
    "    if max_num_words and num_words == max_num_words:\n",
    "      break\n",
    "  if result.syn0.shape[0] != len(result.vocab):\n",
    "    print(\n",
    "        \"duplicate words detected, shrinking matrix size from %i to %i\",\n",
    "        result.syn0.shape[0], len(result.vocab))\n",
    "  result.syn0 = np.ascontiguousarray(result.syn0[:len(result.vocab)])\n",
    "  assert (len(result.vocab), vector_size) == result.syn0.shape\n",
    "\n",
    "  print(\"loaded %s matrix\", result.syn0.shape)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNAARuwKGHu5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize the embeddings client if this hasn't been done yet.\n",
    "# For the efficiency of this notebook we just load the first 2M words, and don't\n",
    "# re-initialize the client if it already exists. You could of course filter the\n",
    "# word list in other ways.\n",
    "if not 'client' in vars():\n",
    "  print \"Loading word embeddings from %s\" % WORD2VEC_FILE\n",
    "  with gzip.GzipFile(fileobj=open(WORD2VEC_FILE, 'r')) as f:\n",
    "    client = load_word2vec_format(f, max_num_words=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_N47nk687UG"
   },
   "source": [
    "The following blocks load a data file with analogy training examples and displays some of them as examples.  By changing the indices selected in the final block you can change which analogies from the training set are being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKGlGo5VJnU4"
   },
   "outputs": [],
   "source": [
    "def print_knn(client, v, k):\n",
    "  print \"%d closest neighbors to A-B+C:\" % k\n",
    "  for neighbor, score in client.similar_by_vector(\n",
    "      v.flatten().astype(float), topn=k):\n",
    "    print \"%s : score=%f\" % (neighbor, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eG_tc8kOkMlm"
   },
   "source": [
    "Let's take a look at the analogies that the model generates for *man*:*woman*::*boss*:$\\underline{\\quad}$.\n",
    "Try changing ``\"boss\"`` to ``\"friend\"`` to see further examples of problematic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rEfwqtDIt0Q"
   },
   "outputs": [],
   "source": [
    "# Use a word embedding to compute an analogy\n",
    "# Edit the parameters below to get different analogies\n",
    "A = \"man\"\n",
    "B = \"woman\"\n",
    "C = \"boss\"\n",
    "NUM_ANALOGIES = 5\n",
    "\n",
    "in_arr = []\n",
    "for i, word in enumerate((A, B, C)):\n",
    "  in_arr.append(client.word_vec(word))\n",
    "in_arr = np.array([in_arr])\n",
    "\n",
    "print_knn(client, -in_arr[0, 0, :] + in_arr[0, 1, :] + in_arr[0, 2, :],\n",
    "          NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLhtOKQKKN4W"
   },
   "outputs": [],
   "source": [
    "def load_analogies(filename):\n",
    "  \"\"\"Loads analogies.\n",
    "\n",
    "  Args:\n",
    "    filename: the file containing the analogies.\n",
    "\n",
    "  Returns:\n",
    "    A list containing the analogies.\n",
    "  \"\"\"\n",
    "  analogies = []\n",
    "  with open(filename, \"r\") as fast_file:\n",
    "    for line in fast_file:\n",
    "      line = line.strip()\n",
    "      # in the analogy file, comments start with :\n",
    "      if line[0] == \":\":\n",
    "        continue\n",
    "      words = line.split()\n",
    "      # there are no misformatted lines in the analogy file, so this should\n",
    "      # only happen once we're done reading all analogies.\n",
    "      if len(words) != 4:\n",
    "        print \"Invalid line: %s\" % line\n",
    "        continue\n",
    "      analogies.append(words)\n",
    "  print \"loaded %d analogies\" % len(analogies)\n",
    "  return analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RtfKMshE0gy"
   },
   "outputs": [],
   "source": [
    "analogies = load_analogies(ANALOGIES_FILE)\n",
    "print \"\\n\".join(\"%s is to %s as %s is to %s\" % tuple(x) for x in analogies[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pM8NTb7bKo_5"
   },
   "outputs": [],
   "source": [
    "def _np_normalize(v):\n",
    "  \"\"\"Returns the input vector, normalized.\"\"\"\n",
    "  return v / np.linalg.norm(v)\n",
    "\n",
    "\n",
    "def load_vectors(client, analogies):\n",
    "  \"\"\"Loads and returns analogies and embeddings.\n",
    "\n",
    "  Args:\n",
    "    client: the client to query.\n",
    "    analogies: a list of analogies.\n",
    "\n",
    "  Returns:\n",
    "    A tuple with:\n",
    "    - the embedding matrix itself\n",
    "    - a dictionary mapping from strings to their corresponding indices\n",
    "      in the embedding matrix\n",
    "    - the list of words, in the order they are found in the embedding matrix\n",
    "  \"\"\"\n",
    "  words_unfiltered = set()\n",
    "  for analogy in analogies:\n",
    "    words_unfiltered.update(analogy)\n",
    "  print \"found %d unique words\" % len(words_unfiltered)\n",
    "\n",
    "  vecs = []\n",
    "  words = []\n",
    "  index_map = {}\n",
    "  for word in words_unfiltered:\n",
    "    try:\n",
    "      vecs.append(_np_normalize(client.word_vec(word)))\n",
    "      index_map[word] = len(words)\n",
    "      words.append(word)\n",
    "    except KeyError:\n",
    "      print \"word not found: %s\" % word\n",
    "  print \"words not filtered out: %d\" % len(words)\n",
    "\n",
    "  return np.array(vecs), index_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TLg0wygKKW0"
   },
   "outputs": [],
   "source": [
    "embed, indices, words = load_vectors(client, analogies)\n",
    "\n",
    "embed_dim = len(embed[0].flatten())\n",
    "print \"word embedding dimension: %d\" % embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFSGOaqDLPij"
   },
   "outputs": [],
   "source": [
    "def find_gender_direction(embed,\n",
    "                          indices):\n",
    "  \"\"\"Finds and returns a 'gender direction'.\"\"\"\n",
    "  pairs = [\n",
    "      (\"woman\", \"man\"),\n",
    "      (\"her\", \"his\"),\n",
    "      (\"she\", \"he\"),\n",
    "      (\"aunt\", \"uncle\"),\n",
    "      (\"niece\", \"nephew\"),\n",
    "      (\"daughters\", \"sons\"),\n",
    "      (\"mother\", \"father\"),\n",
    "      (\"daughter\", \"son\"),\n",
    "      (\"granddaughter\", \"grandson\"),\n",
    "      (\"girl\", \"boy\"),\n",
    "      (\"stepdaughter\", \"stepson\"),\n",
    "      (\"mom\", \"dad\"),\n",
    "  ]\n",
    "  m = []\n",
    "  for wf, wm in pairs:\n",
    "    m.append(embed[indices[wf]] - embed[indices[wm]])\n",
    "  m = np.array(m)\n",
    "\n",
    "  # the next three lines are just a PCA.\n",
    "  m = np.cov(np.array(m).T)\n",
    "  evals, evecs = np.linalg.eig(m)\n",
    "  return _np_normalize(np.real(evecs[:, np.argmax(evals)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSj8daFnKvNr"
   },
   "outputs": [],
   "source": [
    "# Using the embeddings, find the gender vector.\n",
    "gender_direction = find_gender_direction(embed, indices)\n",
    "print \"gender direction: %s\" % str(gender_direction.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVJIOmh2LcV7"
   },
   "source": [
    " The code below illustrates how to construct a function which computes $Z$ from $X$ in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpOpO6RGBznY"
   },
   "source": [
    "Try editing the WORD param in the next cell to see the projection of other words onto the gender direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BipW3yjvLJFz"
   },
   "outputs": [],
   "source": [
    "WORD = \"she\"\n",
    "\n",
    "word_vec = client.word_vec(WORD)\n",
    "print word_vec.dot(gender_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlxOV3RZBpou"
   },
   "source": [
    "Let's now look at the words with the largest *negative* projection onto the gender dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxjWqgCXLe6S"
   },
   "outputs": [],
   "source": [
    "words = set()\n",
    "for a in analogies:\n",
    "  words.update(a)\n",
    "\n",
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"gender_score\"] = df[\"word\"].map(\n",
    "    lambda w: client.word_vec(w).dot(gender_direction))\n",
    "df.sort_values(by=\"gender_score\", inplace=True)\n",
    "print df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9fx985BByiU"
   },
   "source": [
    "Let's now look at the words with the largest *positive* projection onto the gender dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTLLr12vB2mN"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=\"gender_score\", inplace=True, ascending=False)\n",
    "print df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMEGkmSlTsR6"
   },
   "source": [
    "### Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tsGeQLKT3ZY"
   },
   "outputs": [],
   "source": [
    "def tf_normalize(x):\n",
    "  \"\"\"Returns the input vector, normalized.\n",
    "\n",
    "  A small number is added to the norm so that this function does not break when\n",
    "  dealing with the zero vector (e.g. if the weights are zero-initialized).\n",
    "\n",
    "  Args:\n",
    "    x: the tensor to normalize\n",
    "  \"\"\"\n",
    "  return x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "\n",
    "class AdversarialEmbeddingModel(object):\n",
    "  \"\"\"A model for doing adversarial training of embedding models.\"\"\"\n",
    "\n",
    "  def __init__(self, client,\n",
    "               data_p, embed_dim, projection,\n",
    "               projection_dims, pred):\n",
    "    \"\"\"Creates a new AdversarialEmbeddingModel.\n",
    "\n",
    "    Args:\n",
    "      client: The (possibly biased) embeddings.\n",
    "      data_p: Placeholder for the data.\n",
    "      embed_dim: Number of dimensions used in the embeddings.\n",
    "      projection: The space onto which we are \"projecting\".\n",
    "      projection_dims: Number of dimensions of the projection.\n",
    "      pred: Prediction layer.\n",
    "    \"\"\"\n",
    "    # load the analogy vectors as well as the embeddings\n",
    "    self.client = client\n",
    "    self.data_p = data_p\n",
    "    self.embed_dim = embed_dim\n",
    "    self.projection = projection\n",
    "    self.projection_dims = projection_dims\n",
    "    self.pred = pred\n",
    "\n",
    "  def nearest_neighbors(self, sess, in_arr,\n",
    "                        k):\n",
    "    \"\"\"Finds the nearest neighbors to a vector.\n",
    "\n",
    "    Args:\n",
    "      sess: Session to use.\n",
    "      in_arr: Vector to find nearest neighbors to.\n",
    "      k: Number of nearest neighbors to return\n",
    "    Returns:\n",
    "      List of up to k pairs of (word, score).\n",
    "    \"\"\"\n",
    "    v = sess.run(self.pred, feed_dict={self.data_p: in_arr})\n",
    "    return self.client.similar_by_vector(v.flatten().astype(float), topn=k)\n",
    "\n",
    "  def write_to_file(self, sess, f):\n",
    "    \"\"\"Writes a model to disk.\"\"\"\n",
    "    np.savetxt(f, sess.run(self.projection))\n",
    "\n",
    "  def read_from_file(self, sess, f):\n",
    "    \"\"\"Reads a model from disk.\"\"\"\n",
    "    loaded_projection = np.loadtxt(f).reshape(\n",
    "        [self.embed_dim, self.projection_dims])\n",
    "    sess.run(self.projection.assign(loaded_projection))\n",
    "\n",
    "  def fit(self,\n",
    "          sess,\n",
    "          data,\n",
    "          data_p,\n",
    "          labels,\n",
    "          labels_p,\n",
    "          protect,\n",
    "          protect_p,\n",
    "          gender_direction,\n",
    "          pred_learning_rate,\n",
    "          protect_learning_rate,\n",
    "          protect_loss_weight,\n",
    "          num_steps,\n",
    "          batch_size,\n",
    "          debug_interval=1000):\n",
    "    \"\"\"Trains a model.\n",
    "\n",
    "    Args:\n",
    "      sess: Session.\n",
    "      data: Features for the training data.\n",
    "      data_p: Placeholder for the features for the training data.\n",
    "      labels: Labels for the training data.\n",
    "      labels_p: Placeholder for the labels for the training data.\n",
    "      protect: Protected variables.\n",
    "      protect_p: Placeholder for the protected variables.\n",
    "      gender_direction: The vector from find_gender_direction().\n",
    "      pred_learning_rate: Learning rate for predicting labels.\n",
    "      protect_learning_rate: Learning rate for protecting variables.\n",
    "      protect_loss_weight: The constant 'alpha' found in\n",
    "          debias_word_embeddings.ipynb.\n",
    "      num_steps: Number of training steps.\n",
    "      batch_size: Number of training examples in each step.\n",
    "      debug_interval: Frequency at which to log performance metrics during\n",
    "          training.\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        data_p: data,\n",
    "        labels_p: labels,\n",
    "        protect_p: protect,\n",
    "    }\n",
    "    # define the prediction loss\n",
    "    pred_loss = tf.losses.mean_squared_error(labels_p, self.pred)\n",
    "\n",
    "    # compute the prediction of the protected variable.\n",
    "    # The \"trainable\"/\"not trainable\" designations are for the predictor. The\n",
    "    # adversary explicitly specifies its own list of weights to train.\n",
    "    protect_weights = tf.get_variable(\n",
    "        \"protect_weights\", [self.embed_dim, 1], trainable=False)\n",
    "    protect_pred = tf.matmul(self.pred, protect_weights)\n",
    "    protect_loss = tf.losses.mean_squared_error(protect_p, protect_pred)\n",
    "\n",
    "    pred_opt = tf.train.AdamOptimizer(pred_learning_rate)\n",
    "    protect_opt = tf.train.AdamOptimizer(protect_learning_rate)\n",
    "\n",
    "    protect_grad = {v: g for (g, v) in pred_opt.compute_gradients(protect_loss)}\n",
    "    pred_grad = []\n",
    "\n",
    "    # applies the gradient expression found in the document linked\n",
    "    # at the top of this file.\n",
    "    for (g, v) in pred_opt.compute_gradients(pred_loss):\n",
    "      unit_protect = tf_normalize(protect_grad[v])\n",
    "      # the two lines below can be commented out to train without debiasing\n",
    "      g -= tf.reduce_sum(g * unit_protect) * unit_protect\n",
    "      g -= protect_loss_weight * protect_grad[v]\n",
    "      pred_grad.append((g, v))\n",
    "      pred_min = pred_opt.apply_gradients(pred_grad)\n",
    "\n",
    "    # compute the loss of the protected variable prediction.\n",
    "    protect_min = protect_opt.minimize(protect_loss, var_list=[protect_weights])\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    step = 0\n",
    "    while step < num_steps:\n",
    "      # pick samples at random without replacement as a minibatch\n",
    "      ids = np.random.choice(len(data), batch_size, False)\n",
    "      data_s, labels_s, protect_s = data[ids], labels[ids], protect[ids]\n",
    "      sgd_feed_dict = {\n",
    "          data_p: data_s,\n",
    "          labels_p: labels_s,\n",
    "          protect_p: protect_s,\n",
    "      }\n",
    "\n",
    "      if not step % debug_interval:\n",
    "        metrics = [pred_loss, protect_loss, self.projection]\n",
    "        metrics_o = sess.run(metrics, feed_dict=feed_dict)\n",
    "        pred_loss_o, protect_loss_o, proj_o = metrics_o\n",
    "        # log stats every so often: number of steps that have passed,\n",
    "        # prediction loss, adversary loss\n",
    "        print(\"step: %d; pred_loss_o: %f; protect_loss_o: %f\" % (step,\n",
    "                     pred_loss_o, protect_loss_o))\n",
    "        for i in range(proj_o.shape[1]):\n",
    "          print(\"proj_o: %f; dot(proj_o, gender_direction): %f)\" %\n",
    "                       (np.linalg.norm(proj_o[:, i]),\n",
    "                       np.dot(proj_o[:, i].flatten(), gender_direction)))\n",
    "      sess.run([pred_min, protect_min], feed_dict=sgd_feed_dict)\n",
    "      step += 1\n",
    "      \n",
    "def filter_analogies(analogies,\n",
    "                     index_map):\n",
    "  filtered_analogies = []\n",
    "  for analogy in analogies:\n",
    "    if filter(index_map.has_key, analogy) != analogy:\n",
    "      print \"at least one word missing for analogy: %s\" % analogy\n",
    "    else:\n",
    "      filtered_analogies.append(map(index_map.get, analogy))\n",
    "  return filtered_analogies\n",
    "\n",
    "def make_data(\n",
    "    analogies, embed,\n",
    "    gender_direction):\n",
    "  \"\"\"Preps the training data.\n",
    "\n",
    "  Args:\n",
    "    analogies: a list of analogies\n",
    "    embed: the embedding matrix from load_vectors\n",
    "    gender_direction: the gender direction from find_gender_direction\n",
    "\n",
    "  Returns:\n",
    "    Three numpy arrays corresponding respectively to the input, output, and\n",
    "    protected variables.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  labels = []\n",
    "  protect = []\n",
    "  for analogy in analogies:\n",
    "    # the input is just the word embeddings of the first three words\n",
    "    data.append(embed[analogy[:3]])\n",
    "    # the output is just the word embeddings of the last word\n",
    "    labels.append(embed[analogy[3]])\n",
    "    # the protected variable is the gender component of the output embedding.\n",
    "    # the extra pair of [] is so that the array has the right shape after\n",
    "    # it is converted to a numpy array.\n",
    "    protect.append([np.dot(embed[analogy[3]], gender_direction)])\n",
    "  # Convert all three to numpy arrays, and return them.\n",
    "  return tuple(map(np.array, (data, labels, protect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DeRq6L9BiTo"
   },
   "source": [
    "Edit the training parameters below to experiment with different training runs.\n",
    "\n",
    "For example, try increasing the number of training steps to 50k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhOH62wAR-wz"
   },
   "outputs": [],
   "source": [
    "# Edit the training parameters below to experiment with different training runs.\n",
    "# For example, try \n",
    "pred_learning_rate = 2**-16\n",
    "protect_learning_rate = 2**-16\n",
    "protect_loss_weight = 1.0\n",
    "num_steps = 10000\n",
    "batch_size = 1000\n",
    "\n",
    "embed_dim = 300\n",
    "projection_dims = 1\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('var_scope', reuse=tf.AUTO_REUSE):\n",
    "    analogy_indices = filter_analogies(analogies, indices)\n",
    "\n",
    "    data, labels, protect = make_data(analogy_indices, embed, gender_direction)\n",
    "    data_p = tf.placeholder(tf.float32, shape=[None, 3, embed_dim], name=\"data\")\n",
    "    labels_p = tf.placeholder(tf.float32, shape=[None, embed_dim], name=\"labels\")\n",
    "    protect_p = tf.placeholder(tf.float32, shape=[None, 1], name=\"protect\")\n",
    "\n",
    "    # projection is the space onto which we are \"projecting\". By default, this is\n",
    "    # one-dimensional, but this can be tuned by projection_dims\n",
    "    projection = tf.get_variable(\"projection\", [embed_dim, projection_dims])\n",
    "\n",
    "    # build the prediction layer\n",
    "    # pred is the simple computation of d = -a + b + c for a : b :: c : d\n",
    "    pred = -data_p[:, 0, :] + data_p[:, 1, :] + data_p[:, 2, :]\n",
    "    pred -= tf.matmul(tf.matmul(pred, projection), tf.transpose(projection))\n",
    "\n",
    "    trained_model = AdversarialEmbeddingModel(\n",
    "        client, data_p, embed_dim, projection, projection_dims, pred)\n",
    "\n",
    "    trained_model.fit(sess, data, data_p, labels, labels_p, protect, protect_p, gender_direction,\n",
    "              pred_learning_rate,\n",
    "            protect_learning_rate, protect_loss_weight, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt--_4UCWTIg"
   },
   "source": [
    "### Analogy generation using the embeddings with bias reduced by the adversarial model\n",
    "\n",
    "Let's see how the model that has been trained to mitigate bias performs on the analogy task.\n",
    "As before, change \"boss\" to \"friend\" to see how those analogies have changed too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_56sPusFUVQP"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "A = \"man\"\n",
    "B = \"woman\"\n",
    "C = \"boss\"\n",
    "NUM_ANALOGIES = 5\n",
    "\n",
    "# Use a word embedding to compute an analogy\n",
    "in_arr = []\n",
    "for i, word in enumerate((A, B, C)):\n",
    "  in_arr.append(client.word_vec(word))\n",
    "in_arr = np.array([in_arr])\n",
    "\n",
    "print_knn(client, sess.run(pred, feed_dict={data_p: in_arr}),\n",
    "          NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Ei_ioWT_DDa"
   },
   "source": [
    "##Conclusion\n",
    "\n",
    "The method demonstrated here helps to reduce the amount of bias in word embeddings and, although not demonstrated here, generalizes quite well to other domains and tasks.  By trying to hide a protected variable from an adversary, a machine learned system can reduce the amount of biased information about that protected variable implicit in the system.  In addition to the specific method demonstrated here there are many variations on this theme which can be used to achieve different degrees and types of debiasing.  For example, you could debias with respect to more than one principle component of the protected variable by having the adverary predict multiple projections.  Many other elaborations on this basic idea are possible and hopefully this relatively simple system can serve as the basis for more complex and sophisticated systems capable of achieving subtle types of bias mitigation in many applications."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JndnmDMp66FL"
   ],
   "name": "Mitigating Unwanted Biases in Word Embeddings with Adversarial Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
